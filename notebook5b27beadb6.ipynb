{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af6184af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "# !pip install -q -U bitsandbytes\n",
    "# !pip install --upgrade -q -U transformers\n",
    "# !pip install -q -U peft\n",
    "# !pip install -q -U accelerate\n",
    "# !pip install -q -U datasets\n",
    "# !pip install -q -U git+https://github.com/huggingface/trl.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad80efb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\ammar\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Replace 'your_token_here' with the actual token you copied\n",
    "login('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')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a317afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ammar\\anaconda3\\envs\\llm\\lib\\site-packages\\torch\\cuda\\__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 9010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ..\\c10\\cuda\\CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "No CUDA runtime is found, using CUDA_HOME='C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.5'\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'top_k_top_p_filtering' from 'transformers' (C:\\Users\\ammar\\anaconda3\\envs\\llm\\lib\\site-packages\\transformers\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LoraConfig, PeftModel, prepare_model_for_kbit_training\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtrl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ORPOTrainer, ORPOConfig\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llm\\lib\\site-packages\\trl\\__init__.py:5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# flake8: noqa\u001b[39;00m\n\u001b[0;32m      3\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.4.7\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m set_seed\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BestOfNSampler\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimport_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_peft_available\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llm\\lib\\site-packages\\trl\\core.py:23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pad_sequence\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m top_k_top_p_filtering\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Mapping\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'top_k_top_p_filtering' from 'transformers' (C:\\Users\\ammar\\anaconda3\\envs\\llm\\lib\\site-packages\\transformers\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import torch, multiprocessing\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
    "from trl import ORPOTrainer, ORPOConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecd996d",
   "metadata": {},
   "outputs": [],
   "source": [
    "major_version, minor_version = torch.cuda.get_device_capability()\n",
    "if major_version >= 8:\n",
    "    !pip install flash-attn\n",
    "    torch_dtype = torch.bfloat16\n",
    "    attn_implementation='flash_attention_2'\n",
    "    print(\"Your GPU is compatible with FlashAttention and bfloat16.\")\n",
    "else:\n",
    "    torch_dtype = torch.float16\n",
    "    attn_implementation='eager'\n",
    "    print(\"Your GPU is not compatible with FlashAttention and bfloat16.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bece5f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mistralai/Mistral-7B-v0.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02aa02ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, add_eos_token=True, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6762aa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"HuggingFaceH4/ultrafeedback_binarized\", split=[\"train_prefs\", \"test_prefs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e6e659",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84e21a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(row):\n",
    "    row[\"chosen\"] = tokenizer.apply_chat_template(row[\"chosen\"], tokenize=False)\n",
    "    row[\"rejected\"] = tokenizer.apply_chat_template(row[\"rejected\"], tokenize=False)\n",
    "    return row\n",
    "\n",
    "dataset[0] = dataset[0].map(\n",
    "    process,\n",
    "    num_proc= multiprocessing.cpu_count(),\n",
    "    load_from_cache_file=False,\n",
    ")\n",
    "\n",
    "dataset[1] = dataset[1].map(\n",
    "    process,\n",
    "    num_proc= multiprocessing.cpu_count(),\n",
    "    load_from_cache_file=False,\n",
    ")\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66f99dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type = \"nf4\",\n",
    "    bnb_4bit_compute_dtype = torch_dtype,\n",
    "    bnb_4bit_use_double_quant = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f7992c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch_dtype,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map = {\"\":0},\n",
    "    attn_implementation = attn_implementation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3532814",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b05eb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0.05,\n",
    "    r = 16,\n",
    "    bias = \"none\",\n",
    "    task_type = \"CAUSAL_LM\",\n",
    "    target_modules = ['k_proj', 'q_proj', 'v_proj', 'o_proj', 'gate_proj', 'down_proj', 'up_proj']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae37f959",
   "metadata": {},
   "outputs": [],
   "source": [
    "orpo_config = ORPOConfig(\n",
    "    output_dir=\"./results/\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    do_eval=False,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=2,\n",
    "    log_level=\"debug\",\n",
    "    logging_steps=20,\n",
    "    learning_rate=8e-6,\n",
    "    max_steps=20,\n",
    "    save_steps=10,\n",
    "    save_strategy='epoch',\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    beta=0.1, #beta is ORPO's lambda\n",
    "    max_length=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a1e298",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = ORPOTrainer(\n",
    "    model = model,\n",
    "    train_dataset = dataset[0],\n",
    "    eval_dataset = dataset[1],\n",
    "    peft_config = peft_config,\n",
    "    args = orpo_config,\n",
    "    tokenizer = tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8327e00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfdfee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26d4cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "# Run text generation pipeline with our next model\n",
    "prompt = \"What is a large language model?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5d5dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474c151f",
   "metadata": {},
   "outputs": [],
   "source": [
    "username = \"Mora\"\n",
    "MODEL_NAME = \"Mistral7b-ORPO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f3f216",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = HfApi(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ef7353",
   "metadata": {},
   "outputs": [],
   "source": [
    "api.create_repo(\n",
    "    repo_id = f\"{username}/{MODEL_NAME}\",\n",
    "    repo_type=\"model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6c0928",
   "metadata": {},
   "outputs": [],
   "source": [
    "api.upload_folder(\n",
    "    repo_id = f\"{username}/{MODEL_NAME}\",\n",
    "    folder_path = \"results/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00f852d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
